특징 표현(Feature Representation) 생성이라는 더 큰 맥락에서 볼 때, 소스는 자연어를 딥러닝 모델이 처리할 수 있는 **벡터 유형 표현**으로 변환하는 다양한 주요 방식을 제시하고 있으며, 이 방식들은 크게 희소 표현(Sparse Representation)과 밀집 표현(Dense Representation), 그리고 신경망 아키텍처를 활용한 학습 표현으로 나눌 수 있습니다.

### 1. 희소 표현 (Sparse Representation)

희소 표현은 특징 공간에서 단어들 간의 관계를 효율적으로 포착하지 못하며 일반적으로 고차원적이고 계산적으로 비효율적이라는 특징을 가집니다.

*   **원-핫 표현 (One-Hot Representation):**
    *   결과적으로 **매우 고차원적이고 희소한 표현(very high dimensional, very sparse representation)**을 생성합니다.
    *   이 특징 공간에서는 서로 다른 단어들 사이에 **의미 있는 연결(meaningful connection)**이 존재하지 않습니다. 예를 들어, 'ocean'(바다)과 'water'(물)처럼 연관성이 높은 단어 쌍조차도 'ocean'과 'fire'(불)처럼 연관성이 낮은 쌍보다 (표현 공간에서) 더 가깝게 표현되지 않습니다.
*   **Bag of Words (BoW):**
    *   BoW는 단어의 **출현 횟수(number of word appearances)**를 계산합니다.
    *   결과 벡터는 일반적으로 **희소(sparse)**하며, 이는 어휘량이 클 경우 계산적으로 비효율적일 수 있습니다. 또한 어휘의 크기가 매우 커질 수 있습니다.

### 2. 밀집 표현 (Dense Representation)

Word2Vec과 같은 밀집 표현 방식은 희소 표현의 한계를 극복하고 단어를 연속적인 벡터 공간에 효율적으로 표현하여 단어 간의 의미적 및 구문적 유사성을 포착합니다.

*   **Continuous Bag of Words (CBOW):**
    *   주변 **문맥 단어**를 입력으로 받아 목표 단어를 예측하는 방식입니다.
    *   CBOW는 단어를 **연속적인 벡터 공간**에 효율적으로 표현하며, **의미적 및 구문적 유사성**을 포착합니다.
    *   대규모 데이터셋에 대해 효율적이고 확장성이 뛰어납니다.
*   **Skip-gram:**
    *   **하나의 단어**를 입력으로 받아 해당 단어의 주변 **문맥 단어**를 예측하려고 시도합니다.
    *   각 목표 단어에 대해 여러 문맥 단어를 고려하기 때문에 광범위한 단어 관계를 포착하는 데 효율적이며, 특히 **발생 빈도가 낮은 단어**에 대해 고품질 단어 벡터를 생성하는 데 효과적입니다.

### 3. 신경망 아키텍처를 활용한 학습된 표현

일부 신경망 모델은 특정 목표를 달성하기 위해 데이터 자체에서 압축되고 효율적인 특징 표현을 학습합니다.

*   **오토인코더 (Autoencoder):**
    *   데이터의 **효율적인 표현(efficient representations)**을 학습하는 데 사용되는 인공신경망 유형입니다.
    *   **인코더**는 입력을 **잠재 공간 표현(latent-space representation)**으로 압축하며, 이 잠재 공간(Latent space) 레이어는 압축된 입력을 나타내는 코드를 포함합니다.
    *   **디코더**는 이 잠재 코드에 기반하여 입력 데이터를 재구성하려고 시도합니다.
    *   특징적으로, 오토인코더는 입력과 출력에 **동일한 데이터**를 사용하며, 입력과 출력의 **길이도 동일합니다**.
*   **Seq2Seq 아키텍처:**
    *   입력 시퀀스를 출력 시퀀스로 변환하는 인코더-디코더 구조로, 자연어 처리 작업에 적합합니다.
    *   **인코더**는 입력 시퀀스를 **컨텍스트 벡터(context vector)**로 변환하고, 이 컨텍스트 벡터(압축된 입력을 나타내는 코드 포함)를 **디코더**가 새로운 시퀀스를 생성하는 데 사용합니다.
    *   이 방식에서는 입력과 출력의 형태가 다를 수 있으며, 입력과 출력의 길이가 동적으로 다를 수 있습니다.
*   **BERT 입력 표현 (BERT Input Representation):**
    *   Transformer 인코더 기반 모델인 BERT에서 사용되는 특징 표현은 **토큰 임베딩, 세그먼트 임베딩, 위치 임베딩** 세 가지 임베딩의 합으로 구성됩니다.
    *   **토큰 임베딩**은 워드 피스 임베딩(word piece embedding)을 사용하여 단어를 서브워드로 분리함으로써 토큰 표현에 단어를 임베딩합니다.
    *   **세그먼트 임베딩**은 두 입력 문장을 구별하는 정보를 만들며, **위치 임베딩**은 문장 내 각 단어에 위치 정보를 추가합니다.