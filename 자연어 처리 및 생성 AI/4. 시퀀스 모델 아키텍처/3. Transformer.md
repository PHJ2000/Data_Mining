트랜스포머 아키텍처는 시퀀스 모델 아키텍처의 큰 진화 단계로서, 기존의 순환 신경망(RNN) 기반 모델을 대체하고 자연어 처리(NLP) 분야의 **주요 아키텍처(The dominant architecture)**로 자리매김했습니다.

트랜스포머는 RNN 레이어 없이 **오직 어텐션 메커니즘(attention mechanism)만을 활용**하여 시퀀스 데이터를 처리하도록 고안된 신경망 아키텍처입니다.

### 1. 트랜스포머의 핵심 혁신

트랜스포머는 이전 세대 시퀀스 모델인 Seq2Seq 아키텍처가 가졌던 한계, 즉 모든 입력 정보를 고정된 크기의 컨텍스트 벡터로 압축해야 했던 **정보 병목 현상**과 **장기 의존성** 문제를 해결하기 위해 도입된 어텐션 메커니즘을 모델의 유일한 구성 요소로 격상시켰습니다.

*   **RNN 및 CNN 대체:** 트랜스포머 아키텍처는 순환(recurrence)이나 합성곱(convolution)을 사용하지 않기 때문에, 시퀀스 내의 위치 정보를 학습하기 위해 **셀프-어텐션(Self-Attention)** 메커니즘을 활용합니다. 이는 순환 및 합성곱 신경망과 같은 대안적인 신경망 모델을 능가했습니다.

### 2. 시퀀스 처리 요소

트랜스포머는 RNN이나 CNN과 같은 순차적 처리 메커니즘이 없기 때문에, 입력 시퀀스의 순서에 대한 개념을 자체적으로 가지지 못합니다.

*   **위치 인코딩 (Positional Encoding):** 이러한 순서 정보를 모델에 제공하기 위해, **위치 인코딩**이 입력 임베딩에 추가됩니다. 이 인코딩은 각 토큰의 위치에 대한 정보를 모델에 제공하며, 시퀀스 길이가 증가할 때 공간 요구 사항 및 고정 비트 폭의 단점을 해결하기 위해 **사인파(sinusoidal) 위치 인코딩** 방식을 사용합니다.

### 3. 주요 구성 요소

트랜스포머는 인코더와 디코더의 두 가지 주요 블록으로 구성됩니다. 각 블록은 여러 핵심 요소를 사용합니다.

*   **셀프 어텐션 (Self-Attention):** 시퀀스의 표현(representation)을 계산하기 위해 **하나의 시퀀스 내**에서 서로 다른 위치를 연결하는 어텐션 메커니즘입니다.
*   **스케일드 닷-프로덕트 어텐션 (Scaled Dot-Product Attention):** 쿼리(Queries), 키(Keys), 값(Values)을 입력으로 받으며, 큰 $d_k$ 값에 대해 닷 프로덕트(내적)의 크기가 너무 커져 소프트맥스 함수의 기울기가 극도로 작아지는 것을 방지하기 위해 닷 프로덕트 결과를 $1/\sqrt{d_k}$로 **스케일링**합니다.
*   **멀티-헤드 어텐션 (Multi-Head Attention):** 단일 쿼리, 키, 값 세트 대신 여러 세트를 사용합니다. 각 세트는 고유한 학습된 선형 투영(linear projection)을 통해 변환되어 서로 다른 쿼리, 키, 값 세트를 생성합니다. 이를 통해 병렬적인 어텐션 메커니즘으로 컨텍스트 정보를 생성합니다.
*   **포지션별 피드 포워드 네트워크 (Position-wise Feed-Forward Networks):** 인코더와 디코더의 각 레이어에는 어텐션 서브 레이어 외에도 완전히 연결된 피드 포워드 네트워크가 포함되어 있으며, 각 위치에 **개별적으로 동일하게** 적용됩니다. 이 네트워크는 ReLU 활성화 함수를 사이에 두고 두 개의 선형 변환으로 구성됩니다.

### 4. 인코더와 디코더의 역할

트랜스포머는 인코더-디코더 구조를 기반으로 다양한 NLP 작업을 수행합니다.

*   **인코더 (Encoder):** 위치 인코딩 및 멀티-헤드 어텐션 메커니즘을 사용하여 입력 데이터를 컨텍스트 데이터로 **인코딩**합니다. 인코더는 일반적으로 **사전 학습(pre-training)** 및 모델 **미세 조정(fine-tuning)**에 사용됩니다 (예: **BERT**는 트랜스포머 인코더 기반 모델입니다).
*   **디코더 (Decoder):** 인코딩된 입력을 사용하여 타겟 시퀀스를 **생성**합니다. 이 과정에서 디코더는 예측 시점($t$)에서 정보를 제한하기 위해 **마스크드 멀티-헤드 어텐션(Masked Multi-Head Attention)**을 사용하며, 인코더 정보를 사용하여 컨텍스트 정보를 생성하는 멀티-헤드 어텐션을 사용합니다. 디코더는 일반적으로 텍스트 생성, 번역 등을 포함하는 **생성 모델(generative model)**로 사용됩니다 (예: **GPT-4**는 다수의 트랜스포머 디코더를 연결하여 사용합니다).

트랜스포머는 궁극적으로 어텐션 메커니즘을 통해 컨텍스트와 이해력을 학습하는 신경망이며, 텍스트 분류, 기계 번역, 요약 등의 작업에 활용됩니다.