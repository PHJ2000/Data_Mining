Seq2Seq(Sequence-to-Sequence) 아키텍처는 **시퀀스 모델 아키텍처**라는 더 큰 맥락에서, **입력 시퀀스를 출력 시퀀스로 변환**하도록 설계된 핵심적인 모델 구조입니다. 이 모델은 자연어 처리(NLP) 분야에서 기계 번역(machine translation)이나 요약(summarization)과 같은 작업을 위해 활용되는 구조입니다.

### Seq2Seq의 인코더-디코더 구조

Seq2Seq는 데이터 시퀀스를 처리하도록 특화된 순환 신경망(Recurrent Neural Networks, RNN)과 같은 기반 모델을 사용하여 구현되는 경우가 많으며, 근본적으로 인코더-디코더 구조를 따릅니다.

1.  **인코더 (Encoder):** 인코더 레이어는 임베딩 벡터로부터 은닉 벡터(hidden vectors)를 생성하는 역할을 합니다. 인코더의 주요 기능은 전체 입력 시퀀스를 압축하여 하나의 표현으로 변환하는 것입니다.
2.  **컨텍스트 벡터 (Context vector):** 이 레이어는 압축된 입력을 나타내는 코드를 포함합니다. 인코더가 입력 시퀀스를 이 컨텍스트 벡터로 변환하고, 디코더는 이 컨텍스트 벡터를 사용하여 새로운 시퀀스를 생성합니다.
3.  **디코더 (Decoder):** 디코더 레이어는 임베딩 벡터로부터 은닉 벡터를 생성하며, 컨텍스트 벡터를 사용하여 출력 시퀀스를 생성합니다. 문장의 시작 토큰(`<BOS>`)과 문장의 끝 토큰(`<EOS>`)이 디코딩 과정에 사용됩니다.

### 시퀀스 모델로서의 특징

Seq2Seq는 시퀀스 변환 작업에 적합하도록 설계되었으며, 이는 다른 시퀀스 모델(예: 오토인코더)과 구별되는 다음과 같은 특성을 가집니다:

*   **변환 목적:** Seq2Seq는 **하나의 시퀀스를 다른 시퀀스로 변환**하도록 설계되어 NLP 작업에 적합합니다.
*   **데이터 형태 및 길이:** 입력과 출력은 서로 **다른 형태일 수 있습니다**. 또한, 입력 시퀀스와 출력 시퀀스의 길이가 **다를 수 있으며** 동적으로 처리됩니다.

### Seq2Seq 아키텍처의 한계와 발전

Seq2Seq 아키텍처는 NLP에서 매우 유용했음에도 불구하고, 몇 가지 중요한 한계점을 가지고 있었습니다.

1.  **정보 병목 현상 (Information Bottleneck):** 모든 입력 정보를 고정된 크기의 **컨텍스트 벡터**로 압축해야 했기 때문에, 복잡한 입력을 다룰 때 정보의 손실이 발생할 수 있는 병목 현상이 있었습니다.
2.  **장기 의존성 (Long-Term Dependencies):** 긴 시퀀스에서는 컨텍스트 벡터가 정확한 출력을 생성하는 데 필요한 모든 정보를 유지하지 못하는 문제가 있었습니다.

이러한 문제를 해결하기 위해 **어텐션 메커니즘 (Attention Mechanism)**이 도입되었습니다. 어텐션 메커니즘을 사용하면, 디코더는 **전체 입력 시퀀스**에서 필요한 정보에 집중할 수 있게 됩니다. 이 메커니즘을 사용하는 Seq2Seq 모델에서는, 디코더의 각 시간 단계에서 인코더의 모든 은닉 상태에 접근하는 것이 허용됩니다. 그 결과, 디코더는 필요에 따라 전체 입력 시퀀스 중 가장 관련성이 높은 부분에 초점을 맞출 수 있게 됩니다. 어텐션은 쿼리 벡터(Query vector), 키 행렬(Key matrix), 값 행렬(Value matrix) 등을 사용하여 인코더와 디코더 간의 상호작용을 측정하고 필요한 콘텐츠를 추출하는 방식으로 작동합니다.

이러한 개선을 통해 Seq2Seq는 시퀀스 처리 아키텍처로서 성능을 향상시켰으며, 이후 어텐션 메커니즘만을 사용하는 **트랜스포머 아키텍처(Transformer architecture)**의 토대가 되었습니다.