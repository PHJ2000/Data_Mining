**생성 AI(Generative AI)는 오디오, 코드, 이미지, 텍스트, 시뮬레이션, 비디오를 포함하여 **새로운 콘텐츠를 생성하는 기술**을 의미합니다. 이러한 기술은 자연어 처리(NLP)와 딥러닝의 발전 속에서 등장했습니다,.

**LLM(대규모 언어 모델) 맥락에서의 새로운 기술:**

LLM은 생성 AI의 핵심 구성 요소이며, 이 소스에서 언급하는 새로운 콘텐츠 생성 기술은 **트랜스포머(Transformer) 아키텍처**를 기반으로 발전했습니다.

1.  **트랜스포머 아키텍처의 역할:**
    *   트랜스포머는 시퀀스 데이터를 처리하기 위해 **반복 신경망(RNN) 계층 없이 오직 어텐션 메커니즘만을 활용하는 신경망 아키텍처**입니다.
    *   트랜스포머는 대규모 텍스트 말뭉치에 대한 사전 학습에 특히 유리하며, RNN이나 컨볼루션 신경망 같은 다른 신경망 모델을 능가하는 NLP의 지배적인 아키텍처가 되었습니다.
    *   트랜스포머는 어텐션 메커니즘을 통해 문맥과 이해력을 학습합니다.
    *   이러한 트랜스포머 아키텍처는 인코더와 디코더로 구성되는데, 인코더는 입력 데이터를 인코딩하며, 디코더는 인코딩된 입력을 사용하여 목표 시퀀스를 생성합니다.

2.  **생성 AI와 LLM:**
    *   **GPT-4**는 이러한 트랜스포머 구조를 활용하는 대표적인 생성 AI 모델입니다.
    *   GPT-4는 텍스트 대 텍스트 생성 모델로 분류되며, **많은 수의 트랜스포머 디코더를 연결하여 구성**됩니다.
    *   트랜스포머에서 인코더(예: BERT)는 주로 사전 학습과 모델 미세 조정에 사용되는 반면, **디코더(예: GPT-4)는 일반적으로 생성 모델로 사용**됩니다,.
    *   GPT-4는 170억 개 이상의 매개변수를 조정하고 페타바이트(PB) 이상의 데이터를 학습하는 **대규모 언어 모델(LLM)**로 정의됩니다.

3.  **기술적 발전 과정 (NLP 기반):**
    *   딥러닝 모델이 자연어를 학습하기 위해서는 벡터 유형의 표현을 생성해야 합니다.
    *   이러한 시퀀스 데이터 처리를 위해 순환 신경망(Recurrent Neural Networks, RNN)이 사용되었으며, 입력 시퀀스를 출력 시퀀스로 변환하는 **Seq2Seq 아키텍처(인코더-디코더 구조)**가 NLP 작업에 적합하도록 설계되었습니다,.
    *   하지만 Seq2Seq는 모든 입력 정보를 고정된 크기의 문맥 벡터로 압축하는 과정에서 발생하는 정보 병목 현상(Information Bottleneck)과 긴 시퀀스에서 정보가 유지되지 못하는 장기 의존성(Long-Term Dependencies)의 한계를 가졌습니다.
    *   **어텐션 메커니즘**은 디코더가 전체 입력 시퀀스에 걸쳐 필요한 정보에 집중할 수 있도록 함으로써 이러한 한계를 줄였습니다. 트랜스포머는 바로 이 어텐션 메커니즘만을 이용하여 시퀀스 데이터를 처리하도록 설계되어, LLM을 가능하게 한 핵심 기술이 되었습니다.

**생성 AI 사용 시 고려사항:**

새로운 콘텐츠를 생성하기 위해 생성 모델에 명령과 세부 사항을 제공하는 도구를 **프롬프트(Prompt)**라고 합니다. 프롬프트 엔지니어링은 데이터를 생성하기 위해 명확하고 단순하며 유익한 프롬프트를 만드는 과정입니다. 하지만 생성 AI는 학습 데이터의 한계, 복잡하거나 모호한 프롬프트, 현실 세계 이해 부족 등의 이유로 주어진 프롬프트나 예상되는 현실과 일치하지 않는 부정확하거나 무의미한 출력을 생성하는 **환각(Hallucination)** 현상을 일으킬 수 있습니다.