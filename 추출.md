소스에서 예시 또는 example을 통해서 직접 풀어주거나 계산을 한 것을 추출해줘ㅍ

## 연관 규칙 마이닝

요청하신 대로, 소스 내에서 직접 제시된 예시, 데이터 세트, 그리고 계산된 값들을 추출하여 제시합니다.

### 1. 시장 바구니 데이터 예시 (Market-Basket Data Examples)

연관 규칙 마이닝(Association Rule Mining)의 기본이 되는 시장 바구니 모델(Market-Basket Model)은 고객이 한 번의 구매에서 함께 구입한 품목들의 집합을 분석합니다.

**예시 1: 기본적인 거래 데이터**

| TID | 품목 (Items) |
| :--- | :--- |
| 1 | Bread, Coke, Milk |
| 2 | Beer, Bread |
| 3 | Beer, Coke, Diaper, Milk |
| 4 | Beer, Bread, Diaper, Milk |
| 5 | Coke, Diaper, Milk |
이 데이터를 통해 발견된 규칙의 예시는 다음과 같습니다:
*   {Milk} --> {Coke}
*   **{Diaper, Milk} --> {Beer}** (이는 "누군가 기저귀와 우유를 사면 맥주를 살 가능성이 높다"는 유명한 예시를 뒷받침합니다).

**예시 2: 빈번한 품목 집합 계산을 위한 데이터 (B1-B8)**

이 데이터 세트는 지지도(Support)와 신뢰도(Confidence) 계산 예시에 사용됩니다.
*   품목(Items) = {milk (m), coke (c), pepsi (p), beer (b), juice (j)}
*   지지도 임계값 (Support threshold) = 3개 바구니

| 바구니 | 품목 (Items) |
| :--- | :--- |
| B1 | {m, c, b} |
| B2 | {m, p, j} |
| B3 | {m, b} |
| B4 | {c, j} |
| B5 | {m, p, b} |
| B6 | {m, c, b, j} |
| B7 | {c, b, j} |
| B8 | {b, c} |

이 데이터에서 계산된 **빈번한 품목 집합 (Frequent itemsets)**은 다음과 같습니다 (지지도 임계값 3 기준):
{m}, {c}, {b}, {j}, {b,c}, {c,j}, {m,b}

### 2. 지지도 및 신뢰도 계산 예시 (Support and Confidence Calculation Examples)

#### A. 지지도 (Support) 계산

지지도(Support)는 해당 품목 집합이 포함된 바구니의 수입니다.

1.  **예시 (데이터 1 사용, 5개 바구니):**
    *   {Beer, Bread}의 지지도는 **2** 입니다 (TID 2, TID 4에 포함됨).

2.  **예시 (데이터 2 사용, 8개 바구니):**
    *   규칙 $A \rightarrow B$의 지지도는 $A \cup B$ 집합의 지지도와 같습니다.

#### B. 신뢰도 (Confidence) 및 흥미도 (Interest) 계산

신뢰도(Confidence)는 $I \rightarrow j$ 규칙에서 $I \cup j$의 지지도를 $I$의 지지도로 나눈 값입니다 ($conf(I \rightarrow j) = \text{support}(I \cup j) / \text{support}(I)$).

**예시 (데이터 B1-B8 사용):**
*   **연관 규칙: {m, b} → c**
    *   {m, b, c}를 포함하는 바구니 수: B1, B6 (2개).
    *   {m, b}를 포함하는 바구니 수: B1, B3, B5, B6 (4개).
    *   **신뢰도 (Confidence) = 2/4 = 0.5**

*   **흥미도 (Interest) 계산:**
    *   품목 *c*가 나타나는 바구니의 비율: B1, B4, B6, B7, B8 (5개). 전체 8개 중 **5/8**.
    *   흥미도 (Interest) = $| \text{conf}(I \rightarrow j) - \text{Pr}[j] |$
    *   **흥미도 = |0.5 – 5/8| = 1/8** (소스에서는 이 규칙이 그다지 흥미롭지 않다고 판단합니다).

#### C. 규칙 생성 및 신뢰도 계산 예시

수정된 데이터 세트 (10번 출처의 B1-B8, B3가 B3 = {m, c, b, n}으로 변경됨)와 다음 임계값을 사용하여 규칙을 생성합니다.
*   지지도 임계값 $s = 3$
*   신뢰도 임계값 $c = 0.75$

1.  **빈번한 품목 집합 (Frequent itemsets) 추출:**
    {b,m}, {b,c}, {c,m}, {c,j}, {m,c,b}

2.  **규칙 생성 및 신뢰도 계산 예시:**
    *   $b \rightarrow m$: 신뢰도 $c = 4/6$
    *   $b \rightarrow c$: 신뢰도 $c = 5/6$ (5/6 $\approx$ 0.83으로, 신뢰도 임계값 0.75를 만족합니다.)
    *   $b,c \rightarrow m$: 신뢰도 $c = 3/5$
    *   $m \rightarrow b$: 신뢰도 $c = 4/5$ (4/5 = 0.8로, 신뢰도 임계값 0.75를 만족합니다.)
    *   $b,m \rightarrow c$: 신뢰도 $c = 3/4$ (3/4 = 0.75로, 신뢰도 임계값 0.75를 만족합니다.)
    *   $b \rightarrow c,m$: 신뢰도 $c = 3/6$

### 3. A-Priori 알고리즘 단계 예시

A-Priori 알고리즘은 빈번한 $k$ 크기의 품목 집합 ($L_k$)을 찾기 위해 $k-1$ 크기의 빈번한 집합 ($L_{k-1}$)을 사용하여 후보 집합 ($C_k$)을 생성하고 가지치기(Prune)하는 과정을 반복합니다.

**예시: A-Priori 알고리즘의 가상 단계**

1.  **C1 (모든 품목):** { {b} {c} {j} {m} {n} {p} }
2.  **가지치기 후 L1 (빈번한 1-튜플):** { b, c, j, m }
3.  **L1로부터 C2 생성 (후보 2-튜플):** { {b,c} {b,j} {b,m} {c,j} {c,m} {j,m} }
4.  **가지치기 후 L2 (빈번한 2-튜플):** { {b,m} {b,c} {c,m} {c,j} }
5.  **C2로부터 C3 생성 (후보 3-튜플):** { {b,c,m} {b,c,j} {b,m,j} {c,m,j} }
6.  **가지치기 후 L3 (빈번한 3-튜플):** { {b,c,m} }

### 4. 메모리 요구 사항 계산 예시 (Naïve Algorithm)

연관 규칙 마이닝에서 메모리 요구 사항을 보여주는 계산 예시입니다.

*   **가정:** 품목의 개수($N$)가 $10^5$개 (10만 개)이고, 카운트 값에 4-바이트 정수형이 사용된다고 가정합니다.
*   **총 쌍의 수 (Possible Pairs):** $10^5(10^5-1)/2 \approx 5 \times 10^9$ (50억 쌍)
*   **필요한 메모리:**
    *   총 50억 쌍에 4바이트를 곱하면 **$2 \times 10^{10}$ 바이트 (20 기가바이트)**의 메모리가 필요합니다.

이 계산은 품목 집합의 수가 주 메모리 용량을 초과하여 Naïve 접근 방식이 실패할 수 있음을 보여줍니다.

---
마이닝 작업에서 빈번한 품목 집합을 찾는 것은 마치 바닷가에서 보물을 찾는 것과 같습니다. 지지도를 계산하는 것은 **특정 바구니(모래알)에서 특정 품목 집합(보물)을 발견한 횟수를 세는 것**과 같습니다. 신뢰도를 계산하는 것은 **이미 하나의 품목(작은 보물)을 발견했을 때 다른 품목(추가 보물)이 근처에 있을 확률**을 계산하는 것과 유사합니다.



---

## 이상 탐지

제시된 소스에서 이상 탐지(Anomaly Detection)와 관련하여 **직접적인 예시 또는 계산된 수치**를 포함하는 내용은 다음과 같습니다.

### 1. 명확한 계산 결과 (정보 이득)

정보 이론적 접근 방식(Information Theoretic Approaches)에서는 관측치를 삭제할 때 정보가 얼마나 감소하는지를 측정하며, 이상치(Anomalies)는 더 높은 이득(gain)을 보여야 합니다.

*   100명의 참가자에 대한 키와 몸무게 설문조사 예시에서, **마지막 그룹을 제거**했을 때의 정보 이득이 계산되었습니다.
*   모든 다섯 그룹을 포함했을 때의 엔트로피는 **2.08**이었습니다.
*   마지막 그룹을 제거했을 때의 엔트로피는 **1.89**였습니다.
*   따라서 엔트로피 차이(정보 이득)는 **0.19** (**2.08** - **1.89**)로 계산되었습니다.

### 2. 계산된 이상치 점수 (Outlier Scores)

#### 밀도 기반 접근법 (Relative Density-based: LOF)
상대적 밀도 이상치 점수(Local Outlier Factor, LOF) 예시에서, 특정 데이터 포인트에 대해 다음과 같은 이상치 점수가 부여되었습니다:

*   A의 이상치 점수: **6.85**.
*   C의 이상치 점수: **1.33**.
*   D의 이상치 점수: **1.40**.
    *   (이 예시에서 A, C, D는 가장 높은 이상치 점수를 가지며 이상치로 분류됩니다).

#### 클러스터링 기반 접근법 (Distance from Closest Centroids)
가장 가까운 중심점(Centroids)으로부터의 거리를 기준으로 한 이상치 점수 예시에서, 특정 데이터 포인트에 대해 다음과 같은 점수가 부여되었습니다:

*   D의 이상치 점수: **4.6**.
*   C의 이상치 점수: **0.17**.
*   A의 이상치 점수: **1.2**.

### 3. 공식화된 계산 방식

소스에는 직접적인 계산 예시 외에도 이상치를 판별하기 위한 여러 공식이 제시되어 있습니다.

*   **Grubbs’ Test 통계량:** 단변량 데이터에서 이상치를 탐지하는 데 사용되는 Grubbs’ 검정 통계량 $G$는 $G = \frac{\max |X - \bar{X}|}{s}$로 정의됩니다.
*   **재구성 오류 (Reconstruction Error):** 재구성 기반 접근법(Reconstruction-Based Approaches)에서 이상치 점수로 사용되는 재구성 오류는 $Reconstruction Error(x)= x - \hat{x}$로 계산됩니다 (여기서 $\boldsymbol{x}$는 원본 객체, $\hat{\boldsymbol{x}}$는 저차원 공간에서 재구성되어 원본 공간으로 투영된 객체입니다).
*   **가능도 기반 접근법 (Likelihood based approach):** 데이터 $D$의 로그 가능도(log likelihood)를 사용하여, 특정 시점 $t$와 $t+1$ 사이의 차이 $\Delta = LL_t(D) – LL_{t+1}(D)$를 계산합니다. 이 차이가 임계값 $c$보다 클 경우 ($D > c$) 해당 데이터 포인트는 이상치로 선언됩니다.

### 4. 구체적인 사례 예시

계산된 수치는 아니지만, 이상치로서 그 수치가 매우 중요한 맥락을 형성하는 구체적인 예시들도 포함되어 있습니다.

*   **오존층 고갈 역사:** 1985년 남극의 오존 수치가 **정상 수준보다 10% 아래로** 떨어졌으나, 위성 장비는 이 낮은 농도를 이상치로 간주하여 데이터를 폐기했습니다.
*   **데이터 오류 예시:** 이상치의 예시로 **2세 아기가 200파운드**인 경우(데이터 오류) 또는 비정상적으로 키가 큰 사람과 같은 **자연적 변동**이 언급됩니다,.
*   **의료 예시:** 이상치로 간주될 수 있는 사례로 **비정상적으로 높은 혈압**이 있습니다.

이러한 수치들과 예시들은 이상치 탐지 기술이 어떻게 작동하고, 어떤 결과를 도출하며, 데이터에서 이상치가 어떤 맥락적 중요성을 갖는지를 보여줍니다. 이상치 탐지에서 점수(score)를 사용하는 것은 객체를 순위 매길 수 있도록 하며, 이를 통해 해당 개체가 이상치인 정도(degree)를 측정할 수 있습니다. 예를 들어, LOF 점수에서 **6.85**라는 수치는 다른 점수들(**1.33**, **1.40**)보다 해당 데이터 포인트(A)가 훨씬 더 이상치적임을 보여줍니다.

---

## 자연어 처리 및 생성 AI
요청하신 대로 소스에 포함된 계산 수식(공식) 및 구체적인 예시 또는 수치를 정리하여 추출했습니다.

### I. 딥러닝 모델의 기본 계산 및 구조

**1. 퍼셉트론 (Perceptron) 순전파 (Forward Propagation) 수식**

가장 간단한 형태의 신경망인 퍼셉트론의 순전파는 다음과 같이 계산됩니다:

$$
\boldsymbol{𝑭𝒐𝒓𝒘𝒂𝒓𝒅	𝑷𝒓𝒐𝒑𝒂𝒈𝒂𝒕𝒊𝒐𝒏} ∶ \boldsymbol{Y} = \boldsymbol{step} (\boldsymbol{X}\boldsymbol{W}^{\boldsymbol{T}} + \boldsymbol{b}) \text{}
$$

여기서 활성화 함수 $step$은 이진 분류 작업을 위해 다음과 같이 정의됩니다:

$$
\boldsymbol{s}\boldsymbol{t}\boldsymbol{e}\boldsymbol{p} = \begin{cases} 1 & \text{if } \boldsymbol{X}\boldsymbol{W} + \boldsymbol{b} > 0 \\ 0 & \text{else} \end{cases} \text{}
$$

*   입력 데이터는 $\boldsymbol{X} = [x_1, x_2, x_3 	\dots x_n]$ 이고, 출력 데이터 $\boldsymbol{Y}$는 $-1$ 또는 $1$의 값을 가집니다.

**2. 순환 신경망 (RNN) 순전파 수식**

RNN 모델에서 은닉 상태 $h_t$와 출력 $y_t$를 계산하는 수식은 다음과 같습니다:

*   $h_t = \tanh(h_{t-1} \boldsymbol{W}_{hidden}^T + x_t \boldsymbol{W}_{hidden}^T + b_{hidden})$
*   $y_t = h_t \boldsymbol{W}_{out}^T + b_{out}$

활성화 함수인 $\tanh$는 다음과 같이 정의됩니다:

$$
\sigma(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \text{}
$$

### II. 자연어 처리(NLP) 예시 및 수치

**1. NLP 주요 작업 예시**

자연어 처리(NLP)의 하위 분야 작업 예시는 다음과 같습니다:
*   텍스트 분류 (text classification)
*   기계 번역 (machine translation)
*   요약 (summarization)

**2. 피처 표현(Feature Representation) 예시**

*   **불용어 제거 (Stop Words Removal)**의 예시로는 의미론적 중요도가 낮은 단어들인 **'the', 'is', 'in'** 등이 있습니다.
*   **One-Hot Representation**의 한계 예시로, 'ocean'과 'water'처럼 상관관계가 높은 단어 쌍이 'ocean'과 'fire'처럼 상관관계가 낮은 쌍에 비해 표현 공간에서 더 가깝지 않다는 점이 언급됩니다.
*   특징 표현 추출 방법의 예시로는 **Word2vec**, **GloVe**, **FastText**가 있습니다.

**3. BERT 모델 학습 관련 수치 및 예시**

*   BERT (Bidirectional Encoder Representations from Transformer)는 트랜스포머 인코더 기반의 언어 처리 모델입니다.
*   **Masked Language Model (MLM)** 작업 훈련 시, 토큰의 **80%**는 마스킹되고, **10%**는 무작위 단어로 변경되며, **10%**는 그대로 유지됩니다.
*   BERT 입력 표현은 토큰 임베딩, 세그먼트 임베딩, 위치 임베딩 세 가지를 합하여 구성됩니다. 문장의 첫 번째 토큰에는 **[cls]** 토큰이 삽입되며, 두 문장 사이에는 **[sep]** 토큰이 삽입됩니다.
*   BERT는 인코더로 주로 사용되며, GPT-4는 디코더를 연결하여 사용되는 생성 모델의 예시입니다.

**4. 생성형 AI 모델 예시 및 규모**

*   **Text-to-text** 생성 모델의 예시: 텍스트 생성, 번역 등 (**ChatGPT**).
*   **Text-to-image** 생성 모델의 예시: 이미지 생성 및 편집 (**DALL-E**).
*   **Text-to-video** 생성 모델의 예시: 비디오 생성 및 편집 (**Imagen-Video**).
*   **GPT-4**는 **PB 데이터** 이상을 학습하고 **170억 개** 이상의 매개변수를 조정하는 대규모 언어 모델(LLM)입니다.

### III. 트랜스포머 아키텍처 주요 계산

**1. 스케일드 닷-프로덕트 어텐션 (Scaled Dot-Product Attention) 수식**

어텐션 값(Attention Value)은 쿼리 $\boldsymbol{Q}$, 키 $\boldsymbol{K}$, 값 $\boldsymbol{V}$를 사용하여 계산되며, 큰 $d_k$ 값에 대한 문제를 해결하기 위해 $\sqrt{d_k}$로 나누어 스케일링됩니다:

$$
\boldsymbol{A}\boldsymbol{t}\boldsymbol{t}\boldsymbol{e}\boldsymbol{n}\boldsymbol{t}\boldsymbol{i}\boldsymbol{o}\boldsymbol{n}	\boldsymbol{v}\boldsymbol{a}\boldsymbol{l}\boldsymbol{u}\boldsymbol{e} = \text{softmax}(\frac{\boldsymbol{Q}\boldsymbol{K}^T}{\sqrt{d_k}})\boldsymbol{V} \text{}
$$

어텐션 메커니즘에서 에너지 분포는 다음과 같이 계산됩니다:

$$
𝐸𝑛𝑔𝑒𝑔𝑦 = \boldsymbol{Q}\boldsymbol{K}^T \text{}
$$
$$
𝐷𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛	𝑜𝑓	𝐸𝑛𝑔𝑒𝑔𝑦 = \text{softmax}(\boldsymbol{Q}\boldsymbol{K}^T) \text{}
$$

**2. 다중 헤드 어텐션 (Multi-Head Attention) 수식**

다중 헤드 어텐션은 여러 개의 헤드를 병렬적으로 처리한 후 연결하여 최종 출력을 계산합니다:

$$
\boldsymbol{M}\boldsymbol{u}\boldsymbol{l}\boldsymbol{t}\boldsymbol{i}\boldsymbol{H}\boldsymbol{e}\boldsymbol{a}\boldsymbol{d}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \text{Concat}(head_1, \dots, head_h)\boldsymbol{W}^O \text{}
$$

여기서 각 헤드 $head_i$는 다음과 같이 계산됩니다:

$$
head_i = \boldsymbol{A}\boldsymbol{t}\boldsymbol{t}\boldsymbol{e}\boldsymbol{n}\boldsymbol{t}\boldsymbol{i}\boldsymbol{o}\boldsymbol{n}(\boldsymbol{Q}\boldsymbol{W}_i^Q, \boldsymbol{K}\boldsymbol{W}_i^K, \boldsymbol{V}\boldsymbol{W}_i^V) \text{}
$$

**3. 위치별 피드 포워드 네트워크 (Position-wise Feed-Forward Networks) 수식**

트랜스포머의 인코더와 디코더의 각 레이어에 포함된 피드 포워드 네트워크(FFN)는 두 개의 선형 변환과 그 사이에 ReLU 활성화 함수를 사용하여 다음과 같이 계산됩니다:

$$
\boldsymbol{F}\boldsymbol{F}\boldsymbol{N}(x) = \text{max}(0, x\boldsymbol{W}_1 + b_1) \boldsymbol{W}_2 + b_2 \text{}
$$

**4. 사인파 위치 인코딩 (Sinusoidal Positional Encoding) 수식**

트랜스포머에서 토큰의 순서 정보를 제공하기 위해 사용되는 위치 인코딩은 다음과 같은 사인(sine) 및 코사인(cosine) 함수를 이용합니다:

$$
\boldsymbol{PE}(pos, 2i) = \sin(pos/10000^{2i/d_{model}}) \text{}
$$

$$
\boldsymbol{PE}(pos, 2i+1) = \cos(pos/10000^{2i/d_{model}}) \text{}
$$


---

## 추천 시스템

제공된 소스에서 구체적인 예시 또는 계산에 사용된 수치를 포함하는 행렬 구조를 다음과 같이 추출할 수 있습니다.

### 1. Utility Matrix (유틸리티 행렬) 예시

추천 시스템의 핵심 입력 데이터인 **Utility Matrix**는 사용자의 항목에 대한 선호도를 나타내는 값들을 포함합니다. 대부분의 엔트리(값)는 '알 수 없음(unknown)'을 의미하여 행렬이 희소(sparse)한 특징을 가집니다.

| 사용자 | Iron Man | Shang-Chi | Minions | Toy Story |
| :--- | :--- | :--- | :--- | :--- |
| **Jisoo** | **5** | **4** | **4** | (비어있음) |
| **Jennie** | **1** | (비어있음) | (비어있음) | (비어있음) |
| **Rosé** | (비어있음) | **2** | **4** | (비어있음) |
| **Lisa** | (비어있음) | **3** | **5** | (비어있음) |

위 행렬은 사용자들이 영화에 부여한 '알려진(known)' 평점 값의 예시를 보여줍니다. (예: Jisoo는 Iron Man에 5점을 부여했습니다).

### 2. Latent Factor Models 및 Matrix Factorization (행렬 분해) 행렬 예시

행렬 분해(Matrix Factorization)와 잠재 요인 모델(Latent Factor Models) 관련하여, 실제 데이터 또는 분해된 요인을 나타내는 것으로 보이는 수치 행렬의 예시들이 제시되어 있습니다.

#### A. 원래의 평점 행렬 (또는 데이터 일부) 예시

이 행렬은 사용자와 항목 간의 평점을 나타내는 것으로 보이며, 행렬 분해의 입력 또는 그 결과로 복원된 행렬의 근사치를 나타냅니다.

```
45531
312445
53432142
24542
522434
42331
```


#### B. 분해된 요인 행렬 $P^T$ (사용자 요인 행렬) 예시

행렬 분해 $P^T Q$ 구조에서, 이 행렬은 사용자와 잠재 요인(factors) 간의 관계를 나타내는 것으로 보이며, 구체적인 실수 값들로 구성되어 있습니다.

```
.2 -.4 .1
.5 .6 -.5
.5 .3 -.2
.3 2 .1 1.1
-2 2 .1 -.7
.3 .7 -1
```


#### C. 분해된 요인 행렬 $Q$ (항목 요인 행렬) 예시

이 행렬은 항목과 잠재 요인 간의 관계를 나타내며, 역시 구체적인 실수 값들로 구성되어 있습니다.

```
-.9 2 .4 1.4 .3 -.4 .8 -.5 -2.5 .3 -.2 1.1
1.3 -.1 1.2 -.7 2.9 1.4 -1.3 1.4 .5 .7 -.8
.1 -.6 .7 .8 .4 -.3 .9 2.4 1.7 .6 -.4 2.1
```


### 3. Collaborative Filtering (협업 필터링) 계산 공식

직접적인 수치 계산 결과는 없지만, 협업 필터링에서 사용자가 항목 $i$에 대해 부여할 평점 $r_{xi}$를 예측하는 공식이 제시되어 있습니다. 이 공식은 항목 $i$와 유사한 항목들 $j$의 평점 $r_{xj}$을 유사도 $s_{ij}$로 가중 평균하여 평점을 추정하는 방법을 보여줍니다.

$$r_{xi} = \frac{\sum_{j \in N(i;x)} s_{ij} \cdot r_{xj}}{\sum_{j \in N(i;x)} |s_{ij}|}$$


여기서 $s_{ij}$는 항목 $i$와 $j$ 사이의 유사도이며, $N(i; x)$는 사용자 $x$가 평가한 항목 중 $i$와 가장 유사한 $k$개의 이웃 항목을 나타냅니다.